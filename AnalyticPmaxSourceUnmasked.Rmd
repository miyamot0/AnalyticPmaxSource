---
title: "AnalyticPmaxSource"
author: "MASKED"
date: "October 19, 2018"
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(dirname(inputFile),'/README.md')) })
output:
  html_document:
    keep_md: true
---

### Abstract

Research applying the behavioral economic demand framework is increasingly conducted across disciplines, as is research on improving the mathematical accuracy of demand metrics. At present, a variety of methods have been introduced to solve for the point of unit elasticity, or PMAX, in the Exponential model of demand; however, most of these methods vary in their potential for error due to being empirical approximations. Various methods for determining PMAX are presented here and a novel exact solution for PMAX in the Exponential model of demand is introduced. This solution provides an exact calculation of PMAX using the omega function, as algebraic solutions are not possible. This novel approach is introduced, discussed, and systematically compared to earlier methods for determining PMAX using computer simulations. Systematic comparison indicated that this new approach, an exact analytic solution for PMAX, provides results that are identical to computationally-intensive PMAX methods that directly evaluate the slope of the demand function. The exact analytic PMAX approach is reviewed, its calculations explained, and an easy-to-use web tool is provided to assist researchers in easily performing this calculation of PMAX. Implications for reducing potential sources of error are reviewed and future directions are also discussed.

[Markdown Source](https://github.com/miyamot0/AnalyticPmaxSource/blob/master/AnalyticPmaxSource.Rmd)

[Markdown Document](http://htmlpreview.github.com/?https://github.com/miyamot0/AnalyticPmaxSource/blob/master/AnalyticPmaxSource.html)

[Working Sample](http://www.smallnstats.com/index.php?page=PMAX)

### Credits

* Shawn Gilroy, Applied Behavioral Economics Laboratory, Louisiana State University [Github](https://github.com/miyamot0)

* Brent Kaplan, Carilion Research Institute, Virginia Polytechnic Institute and State University [Github](https://github.com/brentkaplan)

* Derek D. Reed, Applied Behavioral Economics Laboratory, University of Kansas (www.behavioraleconlab.com) [Github](https://github.com/derekdreed)

* Donald A. Hantula, Decision Making Laboratory, Temple University [Site](http://astro.temple.edu/~hantula/)

* Steven R. Hursh, Institutes for Behavior Resources, Johns Hopkins University School of Medicine

### Dependencies

- lambertW - Copyright Ben Bolker, port of Omega function from GSL (GPLv3).

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      fig.path = "plots/",
                      dev = c("png"),
                      dpi = 200)

library(beezdemand)
library(optimx)
library(dplyr)
library(tidyr)
library(knitr)

set.seed(65535)           # Seed for consistency
minR2       <- 0.8        # Minimum R2 for inclusion
nPoints     <- 20000      # maximum n series to simulate
nMinSims    <- 1000       # Minimum passing series to proceed
precision   <- 5

# Ben Bolker's port from GSL
# GPLv3
# 
# z = input
# b = branch (principal, by default)
# eps = machine error
# min-imag = imaginary value
lambertW = function(z,b=0,maxiter=10,eps=.Machine$double.eps,
                    min.imag=1e-9) {
  if (any(round(Re(b)) != b))
    stop("branch number for W must be an integer")
  if (!is.complex(z) && any(z<0)) z=as.complex(z)
  ## series expansion about -1/e
  ##
  ## p = (1 - 2*abs(b)).*sqrt(2*e*z + 2);
  ## w = (11/72)*p;
  ## w = (w - 1/3).*p;
  ## w = (w + 1).*p - 1
  ##
  ## first-order version suffices:
  ##
  w = (1 - 2*abs(b))*sqrt(2*exp(1)*z + 2) - 1
  ## asymptotic expansion at 0 and Inf
  ##
  v = log(z + as.numeric(z==0 & b==0)) + 2*pi*b*1i;
  v = v - log(v + as.numeric(v==0))
  ## choose strategy for initial guess
  ##
  c = abs(z + exp(-1));
  c = (c > 1.45 - 1.1*abs(b));
  c = c | (b*Im(z) > 0) | (!Im(z) & (b == 1))
  w = (1 - c)*w + c*v
  ## Halley iteration
  ##
  for (n in 1:maxiter) {
    p = exp(w)
    t = w*p - z
    f = (w != -1)
    t = f*t/(p*(w + f) - 0.5*(w + 2.0)*t/(w + f))
    w = w - t
    if (abs(Re(t)) < (2.48*eps)*(1.0 + abs(Re(w)))
        && abs(Im(t)) < (2.48*eps)*(1.0 + abs(Im(w))))
      break
  }
  if (n==maxiter) warning(paste("iteration limit (",maxiter,
                                ") reached, result of W may be inaccurate",sep=""))
  if (all(Im(w)<min.imag)) w = as.numeric(w)
  return(w)
}

# Does what it says on the tin, calculates pmax
CalculateHurshPmax <- function(Q0_, alpha_, K_) {
  (1/(Q0_ * alpha_ * K_^1.5)) * (0.083 * K_ + 0.65)
}

# Does what it says on the tin, calculates slope using Hursh's derivative equation
CalculateHurshDerivative <- function(price_, Q0_, alpha_, K_) {
  (log((10^K_)) * (-alpha_ * Q0_ * price_ * exp(-alpha_ * Q0_ * price_)))
}

SlopeLossFunction <- function(par, data) {
  abs((log((10^data$K)) * (-data$A * data$Q0 * par[1] * exp(-data$A * data$Q0 * par[1]))) + 1)
}

ObservedPmax <- function(Q0_, alpha_, K_) {
  # Note, prices are in log-units to preserve the log-log comparison
  prices <- seq(-2, 3, 0.001)
  
  # Consumption, with prices expressed with exponential changes
  consumption <- log(Q0_)/log(10) + K_ * (exp(-alpha_ * Q0_ * 10^prices) - 1)
  
  maxOutput <- prices * consumption
  
  10^(prices[which.max(maxOutput)])
}

EmpiricalPmax <- function(dat) {
  tempObj <- dat
  tempObj$PmaxE <- tempObj$x * tempObj$y
  
  tempMax <- max(tempObj$PmaxE, na.rm = TRUE)
  retObj <- tempObj[tempObj$PmaxE == tempMax,]
  
  retObj[1, "x"]
  
}

GetSolution <- function(Q0_, K_, A_) {
  starts <- CalculateHurshPmax(Q0_, A_, K_)
  
  dat <- data.frame(Q0 = Q0_,
                    A = A_,
                    K = K_) 
  
  result <- optimx::optimx(par = c(starts), 
                           fn = SlopeLossFunction,
                           data = dat,
                           method = c("BFGS"),
                           control=list(maxit=2500))
  
  return(result$p1)
}
```
```{r, Simulation, echo=FALSE, cache=TRUE, warning=FALSE}

  sdMap <- 0.5
  
  # Apt data prices, means, sd's
  pricePoints <- c(0,0.25,0.5,1,1.5,2,2.5,3,4,5,6,7,8,9,10,15,20)
  
  # Data means from APT
  consumptionMean <- c(5.856884058,5.425724638,5.257246377,5.049818841,4.714673913,
                       4.413043478,4.081521739,3.685688406,3.151268116,2.674818841,
                       2.16576087,1.799637353,1.424818841,1.113327289,0.923913043,
                       0.490942029,0.350543478)
  
  # Data sd's from APT
  consumptionSD <- c(4.705973278,4.301995389,4.194430187,3.938184029,3.70212462,
                     3.457774816,3.262748553,3.018651843,2.808459372,2.470158851,
                     2.252749078,2.349112958,1.78097839,1.733577022,1.699213558,
                     1.189442317,0.900553209)
  
  # pre-allocate a frame
  preallocatedFrame <- data.frame(matrix(vector(),
                                         nPoints,
                                         length(pricePoints),
                                         dimnames = list(c(),
                                                         c(pricePoints))),
                                  stringsAsFactors = FALSE)
  
  # Naming conventions (they are odd with numerics)
  pricePointsName <- names(preallocatedFrame)
  
  for (i in 1:length(pricePointsName)) {
    # Based on means/sds
    preallocatedFrame[,pricePointsName[i]] <- rnorm(nPoints, mean=consumptionMean[i], sd=sdMap*consumptionSD[i])
  }
  
  # Restore colnames, add row #'s and columns for passes
  colnames(preallocatedFrame) <- pricePoints
  preallocatedFrame$row <- seq(from = 1, to = nrow(preallocatedFrame), by = 1)
  preallocatedFrame$pass <- NA
  preallocatedFrame$R2 <- NA
  preallocatedFrame$Q0d <- NA
  preallocatedFrame$K <- NA
  preallocatedFrame$Alpha <- NA
  preallocatedFrame$HurshPmax <- NA
  
  # Round negatives to flat zero
  tempMat <- as.matrix(preallocatedFrame)
  tempMat[tempMat < 0] <- 0
  preallocatedFrame <- as.data.frame(tempMat)
  
  # Loop through beez to get # passing
  for (i in 1:nrow(preallocatedFrame)) {
    
    #message(paste("Simulating #", i, " of ", nrow(preallocatedFrame), sep = ""))
    
    test <- data.frame(id = rep(preallocatedFrame[i, "row"], length(pricePoints)),
                       x = pricePoints,
                       y = c(unname(unlist(preallocatedFrame[i, 1:17]))))

    preallocatedFrame[i, "pass"] <- beezdemand::CheckUnsystematic(test)$TotalPass
    
    if (preallocatedFrame[i, "pass"] == 3) {
      resPre <- beezdemand::FitCurves(test, equation = "hs", k = "ind", idcol = "id")
      
      if (resPre$R2 > 0.8) {
        preallocatedFrame[i, "R2"] <- resPre$R2
        preallocatedFrame[i, "Q0d"] <- resPre$Q0d
        preallocatedFrame[i, "K"] <- resPre$K
        preallocatedFrame[i, "Alpha"] <- resPre$Alpha
        
        preallocatedFrame[i, "HurshPmax"] <- resPre$Pmaxd
        preallocatedFrame[i, "HurshDerivative"] <- GetSolution(resPre$Q0d, resPre$K, resPre$Alpha)
        preallocatedFrame[i, "ObservedPmax"] <- EmpiricalPmax(test)
        preallocatedFrame[i, "AnalyticPmax"] <- -lambertW(z = -1/log((10^resPre$K))) / (resPre$Alpha * resPre$Q0d) 
      }
      else
      {
        preallocatedFrame[i, "R2"] <- NA
      }
    } else {
      preallocatedFrame[i, "R2"] <- NA
    }
  }
  
#  write.csv(preallocatedFrame, "Results-Simulated Results Pre-Clean.csv")
  passingSeriesFrame = preallocatedFrame[complete.cases(preallocatedFrame),]
#  write.csv(passingSeriesFrame, "Results-Simulated Results Post-Clean.csv")
  
  passingSeriesFrame <- passingSeriesFrame[1:1000,]
```
### Table 1. Distribution of Unit Elasticity Estimates

```{r, Table_1, echo=FALSE, results="asis"}

meltedFrame <- passingSeriesFrame %>%
  select(row, HurshPmax, HurshDerivative, ObservedPmax, AnalyticPmax) %>%
  gather(row) %>%
  group_by(row) %>%
  summarise(Q0 = round(min(value), precision),
            Q1 = round(quantile(value, probs = c(0.25)), precision),
            Q2 = round(median(value), precision),
            Q3 = round(quantile(value, probs = c(0.75)), precision),
            Q4 = round(max(value), precision))

overallR2 <- passingSeriesFrame %>%
  select(R2) %>%
  summarise(Mean = round(mean(R2), 2),
            SD   = round(sd(R2), 2),
            N = n())

kable(meltedFrame, caption = paste("All Series (N=", overallR2$N ,
                                   ", M=", overallR2$Mean, 
                                   ", SD=", overallR2$SD, ")", sep = ""), format = "markdown")

meltedFrame <- passingSeriesFrame %>%
  filter(R2 > 0.9) %>%
  select(row, HurshPmax, HurshDerivative, ObservedPmax, AnalyticPmax) %>%
  gather(row) %>%
  group_by(row) %>%
  summarise(Q0 = round(min(value), precision),
            Q1 = round(quantile(value, probs = c(0.25)), precision),
            Q2 = round(median(value), precision),
            Q3 = round(quantile(value, probs = c(0.75)), precision),
            Q4 = round(max(value), precision))

overallR2 <- passingSeriesFrame %>%
  select(R2) %>%
  filter(R2 > 0.9) %>%
  summarise(Mean = round(mean(R2), 2),
            SD   = round(sd(R2), 2),
            N = n())

kable(meltedFrame, caption = paste("Series with R2 > 0.9 (N=", overallR2$N,
                                   ", M=", overallR2$Mean, 
                                   ", SD=", overallR2$SD, ")", sep = ""), format = "markdown")

```

*: Overall R2 for all series (M = 0.85, SD = 0.03)
**: Series with R2 of .9 or greater (M = 0.92, SD = 0.01)

Note: Quantile distributions for each of the unit elasticity methods.

### Table 2.

```{r, Table_2, echo=FALSE, results="asis"}

corFrame <- passingSeriesFrame %>%
  select(HurshPmax, HurshDerivative, ObservedPmax, AnalyticPmax)

cors <- round(cor(corFrame, method = "spearman"), precision)

kable(cors, caption = "All Series", format = "markdown")

corFrame <- passingSeriesFrame %>%
  filter(R2 > 0.9) %>%
  select(HurshPmax, HurshDerivative, ObservedPmax, AnalyticPmax)

cors <- round(cor(corFrame, method = "spearman"), precision)

kable(cors, caption = "Series with R2 > 0.9", format = "markdown")
```

Note: Spearman correlation matrix for each of the four unit elasticity methods.

### Figure 1. Demand Curve and P<sub>MAX</sub> in Log-Log Space

```{r Figure_1, echo=FALSE, fig.align="center", fig.height=5, fig.width=9}

par(mfrow = c(1, 2))

# These values are used in all demonstrations
Q0 <- 103.24
alpha <- 0.000283586
K <- 1.5

GetDemandConsumption <- function(P, Q0, A, K) {
  log(Q0)/log(10) + K * (exp(-alpha * Q0 * P) - 1)
}

prices <- seq(-1, 2, 0.01)
consumption <- log(Q0)/log(10) + K * (exp(-alpha * Q0 * 10^prices) - 1)

# Normal demand curve plot
plot(prices, 
     consumption,
     main = "Demand Curve in Logarithmic Units",
     yaxt = "n",
     xaxt = "n",
     ylab = "Consumption",
     xlab = "Unit Price",
     type = "l",
     ylim = c(-1, 2),
     xlim = c(-1, 2))

# Custom axis for clarity
atx <- c(-1, 0, 1, 2)
labels <- sapply(atx, function(i) as.expression(bquote(.(10^i))) )
axis(1, at=atx, labels=labels)

# Same as above
aty <- c(-1, 0, 1, 2, 3)
labels2 <- sapply(aty, function(i) as.expression(bquote(.(10^i))) )
axis(2, at=aty, labels=labels2)

abline(v = log10(0.1), col = "lightgray")
abline(h = log10(0.1), col = "lightgray")

for (i in seq(0, 100, by = 10)) {
  abline(v = log10(i), col = "lightgray")
  abline(h = log10(i), col = "lightgray")
}

pMax <- 15.62583
Cons <- GetDemandConsumption(pMax, Q0, alpha, K)

points(log10(pMax), Cons, col = "black")

lines(prices, consumption)

prices <- seq(0.5, 1.8, 0.01)
consumption <- log(Q0)/log(10) + K * (exp(-alpha * Q0 * 10^prices) - 1)

# Normal demand curve plot
plot(prices, 
     consumption,
     main = "Demand at Unit Elasticity",
     yaxt = "n",
     xaxt = "n",
     ylab = "",
     xlab = "Unit Price",
     type = "l",
     lty = 3,
     ylim = c(log10(20), log10(40)),
     xlim = c(log10(10), log10(20)))

# Custom axis for clarity
atx <- c(1, log10(20), log10(30), log10(40), log10(50))
labels <- sapply(atx, function(i) as.expression(bquote(.(10^i))) )
axis(1, at=atx, labels=labels)

# Same as above
aty <- c(1, log10(20), log10(30), log10(40), log10(50))
labels2 <- sapply(aty, function(i) as.expression(bquote(.(10^i))) )
axis(2, at=aty, labels=labels2)

for (i in seq(0, 100, by = 10)) {
  abline(v = log10(i), col = "lightgray")
  abline(h = log10(i), col = "lightgray")
}

points(log10(pMax), Cons, col = "black")

prePmax <- log10(pMax - 0.5)
preCons <- GetDemandConsumption(10^prePmax, Q0, alpha, K)

postPmax <- log10(pMax + 0.5)
postCons <- GetDemandConsumption(10^postPmax, Q0, alpha, K)

yDiff <- preCons - postCons
xDiff <- prePmax - postPmax

points(c(prePmax, postPmax), c(preCons, postCons), col = "gray", pch = 21)

lines(c(prePmax, prePmax), c(preCons - 0.005, postCons))
lines(c(prePmax, postPmax - 0.005), c(postCons, postCons))

text(log10(pMax), log10(24.5),
     cex = 0.75,
     labels = paste("Price Difference: \n", round(postPmax - prePmax, 4), sep = ""))

arrows(log10(pMax), 
       log10(25.5), 
       x1 = log10(pMax), 
       y1 = log10(28),
       code = 2)

text(log10(11.6), log10(28.55), 
     cex = 0.75,
     labels = paste("Consumption Difference: \n", round(postCons - preCons, 4), sep = ""))

arrows(log10(13.55), 
       log10(29), 
       x1 = log10(15), 
       y1 = log10(29),
       code = 2)

text(log10(15.5), log10(35.25), 
     cex = 0.75,
     labels = expression('P'[MAX]*'= -0.0278 / 0.0278 = -1'))

arrows(log10(pMax), 
       log10(34), 
       x1 = log10(pMax), 
       y1 = log10(30),
       code = 2)

#title(xlab = "Unit Price",
#      ylab = "Consumption",
#      outer = TRUE, line = 3)

```

Note: These figures above illustrate the calculation of unit elasticity (PMAX) in log-log space. The left panel illustrates the form of a conventional demand curve and the right panel illustrates the specific calculations involved. As described in the right panel, a slope of -1 on the demand curve indicates that 1 log-unit increase in price equates to a -1 log-unit decrease in levels of consumption.

### Figure 2. Model Slope and Modified Loss Function

```{r Figure_2, echo=FALSE, fig.align="center", fig.height=5, fig.width=9}

par(mfrow = c(1, 2))

# Demand Curve Slope

Q0 <- 103.24
alpha <- 0.000283586
K <- 1.5

prices <- seq(0.1, 35, 0.01)
slope <- CalculateHurshDerivative(prices, Q0, alpha, K)

# Plot of derivative values
plot(prices, slope,
     main = "First Order Derivative",
     xaxt = "n",
     ylab = "Slope",
     xlab = "Unit Price",
     type = "l",
     log = "x")

# Custom axis for clarity
atx <- c(0.1, 1, 10, 100, 1000)
labels <- sapply(atx, function(i) as.expression(bquote(.(i))) )
axis(1, at=atx, labels=labels)

# Arrow to pmax
arrows(5, 
       -1, 
       x1 = 15, 
       y1 = -1,
       code = 2)

# Pmax text
text(2.5, -1,
     cex = 0.75,
     labels = expression('Exact P'[MAX]))
#expression('P'[MAX]*'= -0.0278 / 0.0278 = -1')
text(0.2, -1.2,
     cex = 0.75,
     labels = c("Pmax: 15.6278"))

slope <- abs(slope + 1)

# Plot of modified demand curve
plot(prices, slope,
     main = "Loss Function",
     xaxt = "n",
     ylab = "Loss Value",
     xlab = "Unit Price",
     type = "l",
     log = "x")

# Pretty axes
atx <- c(0.1, 1, 10, 100, 1000)
labels <- sapply(atx, function(i) as.expression(bquote(.(i))) )
axis(1, at=atx, labels=labels)

# Arrow to first zero value
arrows(15.62583,
       0.7,
       x1 = 15.62583,
       y1 = 0.1,
       code = 2)

# Label for derivative-based Pmax
text(15, 0.75,
     cex = 0.75,
     labels = expression('Exact P'[MAX]))

text(0.2, 0.05,
     cex = 0.75,
     labels = c("Pmax: 15.6278"))
```

Note: This figure illustrates a method of solving for PMAX using the first derivative. The left panel illustrates the value of the first derivative (i.e., slope) and the right panel illustrates a modified equation (i.e., a loss function) that represents a slope value of -1 as a zero point, which can then be minimized to determine Exact PMAX at a value of 0.

### Figure 3. Box Plot and Unit Elasticity Distribution

```{r, Figure_3, echo=FALSE, fig.align="center", fig.height=4, fig.width=6}

par(mfrow = c(1, 1), oma = c(0, 0, 0, 0))

compareFrame <- passingSeriesFrame[, c("HurshPmax", "ObservedPmax", "HurshDerivative", "AnalyticPmax")]

# Prepare boxplots, for descriptive summary
suppressMessages(boxPlotData <- reshape2::melt(compareFrame))

boxplot(value~variable,
        data=boxPlotData, 
        main="Distribution of Unit Elasticity Calculations", 
        #ylim = c(0, 100),
        xlab="Unit Elasticity Method", 
        yaxt = "n",
        xaxt = "n",
        ylim = c(1, 100),
        log = "y",
        ylab="Calculation Result")

atx <- 1:4
labels <- c("Approximated", "Observed", "Derivative", "Analytic")
axis(1, at=atx, labels=labels)

aty <- c(1, 10, 100)
labels2 <- sapply(aty, function(i) as.expression(bquote(.(i))) )
axis(2, at=aty, labels=labels2)
```

Note: This figure above illustrates the range of values the range of PMAX values resulting from each type of calculation. The information provided here highlights substantial variability in the Observed method, minimal variability in the Approximate method, and exact correspondence between Derivative and Analytic methods.

### Figure 4. Comparison of Exact and Approximated P<sub>MAX</sub> Methods

```{r, Figure_4, echo=FALSE, fig.align="center", fig.height=3.75, fig.width=9}

bottomMargin   <- 0.4
leftMarginText <- 0.4
leftMarginBlank<- 0.2
topMargin <- 0.2

par(mfrow = c(1, 3), oma = c(0, 0, 2, 0), mar=c(5, 4, 2, 2) + 0.1, omi=c(0.2,0.2,0.4,0))

compareFrame <- passingSeriesFrame[, c("HurshPmax", "ObservedPmax", "HurshDerivative", "AnalyticPmax")]

atx <- c(1, 10, 100)
labels <- sapply(atx, function(i) as.expression(bquote(.(i))) )

aty <- c(1, 10, 100)
labels2 <- sapply(aty, function(i) as.expression(bquote(.(i))) )

# Figure 4-1
par(mai=c(bottomMargin, leftMarginText, topMargin, 0))
plot(compareFrame$ObservedPmax,
     compareFrame$HurshDerivative,
     main = "",
     xlab = "",
     ylab = "",
     xaxt = "n",
     yaxt = "n",
     ylim = c(1, 100),
     xlim = c(1, 100),
     log = "xy",
     type = "p")

axis(1, at=atx, labels=labels)
axis(2, at=aty, labels=labels2)

fitting <- lm(compareFrame$HurshDerivative ~ compareFrame$ObservedPmax)
fitCoefs <- round(coef(fitting), 2)

abline(fitting, col = "lightgray", untf=TRUE)

r2<- summary(fitting)$r.squared

fitInfo <- vector('expression', 2)
fitInfo[1] <- substitute(expression(italic(y) == mIntercept + mCorr * italic(x)),
                         list(mIntercept = format(fitCoefs[1], digits = 3),
                              mCorr      = format(fitCoefs[2], digits = 3)))[2]
fitInfo[2] <- substitute(expression(italic(R)^2 == rSquared),
                         list(rSquared = format(r2, digits = 3)))[2]

legend('topleft', legend = fitInfo, bty = 'n')

mtext(expression('Observed P'[MAX]), 
      side=1, 
      line = 3,
      outer=F, 
      at=10)

# Figure 4-2
par(mai=c(bottomMargin, leftMarginBlank, topMargin, 0))
plot(compareFrame$HurshPmax,
     compareFrame$HurshDerivative,
     main = "",
     xlab = "",
     ylab = "",
     xaxt = "n",
     yaxt = "n",
     ylim = c(1, 100),
     xlim = c(1, 100),
     log = "xy",
     type = "p")

axis(1, at=atx, labels=labels)
axis(2, at=aty, labels=F)

fitting <- lm(compareFrame$HurshDerivative ~ compareFrame$HurshPmax)
fitCoefs <- round(coef(fitting), 2)

abline(lm(compareFrame$HurshDerivative ~ compareFrame$HurshPmax), col = "lightgray", untf=TRUE)

r2<- summary(fitting)$r.squared

fitInfo <- vector('expression', 2)
fitInfo[1] <- substitute(expression(italic(y) == mIntercept + mCorr * italic(x)),
                         list(mIntercept = format(fitCoefs[1], digits = 3),
                              mCorr      = format(fitCoefs[2], digits = 3)))[2]
fitInfo[2] <- substitute(expression(italic(R)^2 == rSquared),
                         list(rSquared = format(r2, digits = 3)))[2]

legend('topleft', legend = fitInfo, bty = 'n')

mtext(expression('Approximate P'[MAX]), 
      side = 1,
      line = 3,
      outer = F,
      at = 10)

par(mai=c(bottomMargin, leftMarginBlank, topMargin, 0))
plot(compareFrame$AnalyticPmax,
     compareFrame$HurshDerivative,
     main = "",
     xlab = "",
     ylab = "",
     xaxt = "n",
     yaxt = "n",
     ylim = c(1, 100),
     xlim = c(1, 100),
     log = "xy",
     type = "p")

axis(1, at=atx, labels=labels)
axis(2, at=aty, labels=F)

fitting <- lm(compareFrame$HurshDerivative ~ compareFrame$AnalyticPmax)
fitCoefs <- round(coef(fitting), 2)

abline(lm(compareFrame$HurshDerivative ~ compareFrame$AnalyticPmax), col = "lightgray", untf=TRUE)

r2<- summary(fitting)$r.squared

fitInfo <- vector('expression', 2)
fitInfo[1] <- substitute(expression(italic(y) == mIntercept + mCorr * italic(x)),
                         list(mIntercept = format(fitCoefs[1], digits = 3),
                              mCorr      = format(fitCoefs[2], digits = 3)))[2]
fitInfo[2] <- substitute(expression(italic(R)^2 == rSquared),
                         list(rSquared = format(r2, digits = 3)))[2]

legend('topleft', legend = fitInfo, bty = 'n')

mtext(expression('Analytic P'[MAX]), 
      side=1, 
      line = 3,
      outer=F, 
      at=10)

mtext(expression(bold('Comparison of Methods for Determining P'[MAX])), outer = T, cex = 1.5)

mtext(expression('Derivative P'[MAX]), side=2, outer=T, at=0.5)
```

Note: This figure illustrates relationships between methods for calculating PMAX. The Observed method, constrained to prices directly measured, varied substantially from methods that directly evaluated the slope demand curve. In contrast, the Approximate method provided a consistent approximation of PMAX and the Analytic method matched exactly with the Derivative method.

### License

Copyright 2018, Shawn P. Gilroy (sgilroy1@lsu.edu)/Louisiana State University - GPLv3
